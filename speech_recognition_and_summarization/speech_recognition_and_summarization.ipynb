{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Recognition And Summarization \n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In this project, we'll build a system that can automatically recognize speech and summarize it. This can be used for automatically transcribing and summarizing lecture recordings, podcasts, or videos.\n",
    "\n",
    "We'll also include a way to hook up a microphone to automatically record and transcribe audio for live notetaking. This could be used to record and transcribe meetings in real-time.\n",
    "\n",
    "By the end of this project, you'll have a speech to text project that you can continue to build on.\n",
    "\n",
    "### Project Steps\n",
    "\n",
    "1. Create a speech recognition system using vosk\n",
    "2. Add punctuation to the text transcript using recasepunc\n",
    "3. Summarize the text using a huggingface summarization pipeline\n",
    "4. Create a widget to record and transcribe live audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ranga/anaconda3/envs/speech/lib/python3.10/site-packages/torch/cuda/__init__.py:83: UserWarning: HIP initialization: Unexpected error from hipGetDeviceCount(). Did you run some cuda functions before calling NumHipDevices() that might have already set an error? Error 101: hipErrorInvalidDevice (Triggered internally at  ../c10/hip/HIPFunctions.cpp:110.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "# import required libraries\n",
    "\n",
    "from vosk import Model, KaldiRecognizer\n",
    "from pydub import AudioSegment\n",
    "import subprocess\n",
    "from transformers import pipeline\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import json\n",
    "import pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOG (VoskAPI:ReadDataFiles():model.cc:213) Decoding params beam=10 max-active=3000 lattice-beam=2\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:216) Silence phones 1:2:3:4:5:6:7:8:9:10\n",
      "LOG (VoskAPI:RemoveOrphanNodes():nnet-nnet.cc:948) Removed 0 orphan nodes.\n",
      "LOG (VoskAPI:RemoveOrphanComponents():nnet-nnet.cc:847) Removing 0 orphan components.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:248) Loading i-vector extractor from /home/ranga/.cache/vosk/vosk-model-small-en-us-0.15/ivector/final.ie\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:204) Done.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:282) Loading HCL and G from /home/ranga/.cache/vosk/vosk-model-small-en-us-0.15/graph/HCLr.fst /home/ranga/.cache/vosk/vosk-model-small-en-us-0.15/graph/Gr.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:303) Loading winfo /home/ranga/.cache/vosk/vosk-model-small-en-us-0.15/graph/phones/word_boundary.int\n"
     ]
    }
   ],
   "source": [
    "FRAME_RATE = 16000\n",
    "CHANNELS=1\n",
    "\n",
    "# model = Model(model_name=\"vosk-model-en-us-0.22\")\n",
    "model = Model(model_name=\"vosk-model-small-en-us-0.15\")\n",
    "# For a smaller download size, use model = Model(model_name=\"vosk-model-small-en-us-0.15\")\n",
    "rec = KaldiRecognizer(model, FRAME_RATE)\n",
    "rec.SetWords(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp3 = AudioSegment.from_mp3(\"marketplace.mp3\")\n",
    "mp3 = mp3.set_channels(CHANNELS)\n",
    "mp3 = mp3.set_frame_rate(FRAME_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec.AcceptWaveform(mp3.raw_data)\n",
    "result = rec.Result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = json.loads(result)[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the funny thing about the big economic news of the day the fed raising interest rates have a percentage point was that there was only really one bit of actual news in the news and the interest rate increase wasn't it you know it was common i know it was common wall street news common businesses knew it was common so on this fed day on this program something a little bit different j powell in his own words five of i'm his most used economic words from today's press conference where number one of course it's the biggie two percent inflation flesh and inflation inflation inflation place in english dealing with inflation bells big worry that thing keeping him up at night price stability is the feds whole ballgame right now pal basically said as much to day or number two\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ranga/anaconda3/envs/speech/lib/python3.10/site-packages/torch/cuda/__init__.py:83: UserWarning: HIP initialization: Unexpected error from hipGetDeviceCount(). Did you run some cuda functions before calling NumHipDevices() that might have already set an error? Error 101: hipErrorInvalidDevice (Triggered internally at  ../c10/hip/HIPFunctions.cpp:110.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "Downloading vocab.txt: 100%|██████████| 226k/226k [00:00<00:00, 700kB/s] \n",
      "Downloading tokenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 12.3kB/s]\n",
      "Downloading config.json: 100%|██████████| 570/570 [00:00<00:00, 55.9kB/s]\n",
      "WARNING: reverting to cpu as cuda is not available\n",
      "Downloading pytorch_model.bin: 100%|██████████| 420M/420M [01:24<00:00, 5.24MB/s] \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "cased = subprocess.check_output('python recasepunc/recasepunc.py predict recasepunc/checkpoint', shell=True, text=True, input=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The funny thing about the big economic news of the day, the Fed raising interest rates have a percentage point was that there was only really one bit of actual news in the news. And the interest rate increase wasn ' t it. You know, it was common. I know it was common Wall Street news. Common businesses knew it was common. So on this Fed day, on this program something a little bit different. J. Powell, in his own words, five of I ' m his most used economic words from today ' s press conference, where number one, Of course, it ' s the biggie Two percent inflation, flesh and inflation inflation inflation Place in English. Dealing with inflation bells. Big worry, that thing keeping him up at night. Price stability is the Feds whole ballgame right now, Pal, Basically said as much to day or number two.\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create speech recognition function\n",
    "\n",
    "def voice_recognition(filename):\n",
    "    # model = Model(model_name=\"vosk-model-en-us-0.22\")\n",
    "    model = Model(model_name=\"vosk-model-small-en-us-0.15\")\n",
    "    # For a smaller download size, use model = Model(model_name=\"vosk-model-small-en-us-0.15\")\n",
    "    rec = KaldiRecognizer(model, FRAME_RATE)\n",
    "    rec.SetWords(True)\n",
    "    \n",
    "    mp3 = AudioSegment.from_mp3(filename)\n",
    "    mp3 = mp3.set_channels(CHANNELS)\n",
    "    mp3 = mp3.set_frame_rate(FRAME_RATE)\n",
    "    \n",
    "    step = 45000\n",
    "    transcript = \"\"\n",
    "    for i in range(0, len(mp3), step):\n",
    "        print(f\"Progress: {i/len(mp3)}\")\n",
    "        segment = mp3[i:i+step]\n",
    "        rec.AcceptWaveform(segment.raw_data)\n",
    "        result = rec.Result()\n",
    "        text = json.loads(result)[\"text\"]\n",
    "        transcript += text\n",
    "    \n",
    "    cased = subprocess.check_output('python recasepunc/recasepunc.py predict recasepunc/checkpoint', shell=True, text=True, input=transcript)\n",
    "    return cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOG (VoskAPI:ReadDataFiles():model.cc:213) Decoding params beam=10 max-active=3000 lattice-beam=2\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:216) Silence phones 1:2:3:4:5:6:7:8:9:10\n",
      "LOG (VoskAPI:RemoveOrphanNodes():nnet-nnet.cc:948) Removed 0 orphan nodes.\n",
      "LOG (VoskAPI:RemoveOrphanComponents():nnet-nnet.cc:847) Removing 0 orphan components.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:248) Loading i-vector extractor from /home/ranga/.cache/vosk/vosk-model-small-en-us-0.15/ivector/final.ie\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:204) Done.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:282) Loading HCL and G from /home/ranga/.cache/vosk/vosk-model-small-en-us-0.15/graph/HCLr.fst /home/ranga/.cache/vosk/vosk-model-small-en-us-0.15/graph/Gr.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:303) Loading winfo /home/ranga/.cache/vosk/vosk-model-small-en-us-0.15/graph/phones/word_boundary.int\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0.0\n",
      "Progress: 0.02666815218151411\n",
      "Progress: 0.05333630436302822\n",
      "Progress: 0.08000445654454233\n",
      "Progress: 0.10667260872605644\n",
      "Progress: 0.13334076090757055\n",
      "Progress: 0.16000891308908466\n",
      "Progress: 0.18667706527059877\n",
      "Progress: 0.21334521745211288\n",
      "Progress: 0.240013369633627\n",
      "Progress: 0.2666815218151411\n",
      "Progress: 0.29334967399665524\n",
      "Progress: 0.3200178261781693\n",
      "Progress: 0.34668597835968346\n",
      "Progress: 0.37335413054119754\n",
      "Progress: 0.4000222827227117\n",
      "Progress: 0.42669043490422576\n",
      "Progress: 0.4533585870857399\n",
      "Progress: 0.480026739267254\n",
      "Progress: 0.5066948914487681\n",
      "Progress: 0.5333630436302822\n",
      "Progress: 0.5600311958117963\n",
      "Progress: 0.5866993479933105\n",
      "Progress: 0.6133675001748246\n",
      "Progress: 0.6400356523563386\n",
      "Progress: 0.6667038045378528\n",
      "Progress: 0.6933719567193669\n",
      "Progress: 0.720040108900881\n",
      "Progress: 0.7467082610823951\n",
      "Progress: 0.7733764132639093\n",
      "Progress: 0.8000445654454234\n",
      "Progress: 0.8267127176269374\n",
      "Progress: 0.8533808698084515\n",
      "Progress: 0.8800490219899657\n",
      "Progress: 0.9067171741714798\n",
      "Progress: 0.9333853263529939\n",
      "Progress: 0.960053478534508\n",
      "Progress: 0.9867216307160221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ranga/anaconda3/envs/speech/lib/python3.10/site-packages/torch/cuda/__init__.py:83: UserWarning: HIP initialization: Unexpected error from hipGetDeviceCount(). Did you run some cuda functions before calling NumHipDevices() that might have already set an error? Error 101: hipErrorInvalidDevice (Triggered internally at  ../c10/hip/HIPFunctions.cpp:110.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "WARNING: reverting to cpu as cuda is not available\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Trancribe bigger mp3 file\n",
    "transcript = voice_recognition(\"marketplace_full.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarization of transcribed audio\n",
    "# summarizer = pipeline(\"summarization\")\n",
    "summarizer = pipeline(\"summarization\", model=\"t5-small\")\n",
    "# For a smaller model, use: summarizer = pipeline(\"summarization\", model=\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_tokens = transcript.split(\" \")\n",
    "docs = []\n",
    "for i in range(0, len(split_tokens), 850):\n",
    "    selection = \" \".join(split_tokens[i:(i+850)])\n",
    "    docs.append(selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = summarizer(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = \"\\n\\n\".join([d[\"summary_text\"] for d in summaries])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Microphone live speech recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find local microphone index\n",
    "\n",
    "p = pyaudio.PyAudio()\n",
    "p.get_device_count()\n",
    "\n",
    "for i in range(p.get_device_count()):\n",
    "    print(p.get_device_info_by_index(i).get('name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to record from microphone\n",
    "\n",
    "def record_microphone(seconds=10, chunk=1024, audio_format=pyaudio.paInt16):\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    stream = p.open(format=audio_format,\n",
    "                    channels=CHANNELS,\n",
    "                    rate=FRAME_RATE,\n",
    "                    input=True,\n",
    "                    input_device_index=2, # match this index to your local microphone index\n",
    "                    frames_per_buffer=chunk)\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    for i in range(0, int(FRAME_RATE / chunk * seconds)):\n",
    "        data = stream.read(chunk)\n",
    "        frames.append(data)\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "    sound = AudioSegment(\n",
    "        data=b''.join(frames),\n",
    "        sample_width=p.get_sample_size(audio_format),\n",
    "        frame_rate=FRAME_RATE,\n",
    "        channels=CHANNELS\n",
    "    )\n",
    "    sound.export(\"temp.mp3\", \"mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_microphone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create buttons for start and stop recording and transcribe/summarize live recording from microphone\n",
    "\n",
    "record_button = widgets.Button(\n",
    "    description='Record',\n",
    "    disabled=False,\n",
    "    button_style='success',\n",
    "    tooltip='Record',\n",
    "    icon='microphone'\n",
    ")\n",
    "\n",
    "summary = widgets.Output()\n",
    "\n",
    "def start_recording(data):\n",
    "    with summary:\n",
    "        display(\"Starting the recording.\")\n",
    "        record_microphone()\n",
    "        display(\"Finished recording.\")\n",
    "        transcript = voice_recognition(\"temp.mp3\")\n",
    "        display(f\"Transcript: {transcript}\")\n",
    "\n",
    "record_button.on_click(start_recording)\n",
    "\n",
    "display(record_button, summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('speech')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "36af68b7e0bd20702ca608d76c52863410261762162ece9a9df47436a8fb3ca2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
