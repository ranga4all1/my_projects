# -*- coding: utf-8 -*-
"""speech_recognition_and_summarization_colab.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19iXGIfJVF6JO6PSvejAy3CefjCFckCa5

# Speech recognition and summarization using colab

## Project Overview

In this project, we'll build a system that can automatically recognize speech and summarize it. This can be used for automatically transcribing and summarizing lecture recordings, podcasts, or videos.

We'll also include a way to hook up a microphone to automatically record and transcribe audio for live notetaking. This could be used to record and transcribe meetings in real-time.

**Project Steps**

1. Create a speech recognition system using vosk
2. Add punctuation to the text transcript using recasepunc
3. Summarize the text using a huggingface summarization pipeline
4. Create a widget to record and transcribe live audio
"""

# Install required libraries
!pip install vosk
!pip install pydub
!pip install transformers
!pip install torch -f https://download.pytorch.org/whl/torch_stable.html
!apt install libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0 ffmpeg
!pip install PyAudio
!pip install ipywidgets

# import required libraries

import os
from google.colab import files
import shutil
import zipfile
from vosk import Model, KaldiRecognizer
from pydub import AudioSegment
import subprocess
from transformers import pipeline
import ipywidgets as widgets
from IPython.display import display
import json
import pyaudio

# vosk speech recognition

FRAME_RATE = 16000
CHANNELS=1

# model = Model(model_name="vosk-model-en-us-0.22")
model = Model(model_name="vosk-model-small-en-us-0.15")
# For a smaller download size, use model = Model(model_name="vosk-model-small-en-us-0.15")
rec = KaldiRecognizer(model, FRAME_RATE)
rec.SetWords(True)

"""- Upload your mp3 files to `content` folder on colab runtime"""

files.upload()

# verify that mp3 files exist

os.listdir('/content')

mp3 = AudioSegment.from_mp3("/content/marketplace.mp3")
mp3 = mp3.set_channels(CHANNELS)
mp3 = mp3.set_frame_rate(FRAME_RATE)

rec.AcceptWaveform(mp3.raw_data)
result = rec.Result()

text = json.loads(result)["text"]

text

# Download recasepunc

!wget --no-check-certificate \
    https://alphacephei.com/vosk/models/vosk-recasepunc-en-0.22.zip \
    -O /content/vosk-recasepunc-en-0.22.zip

local_zip = '/content/vosk-recasepunc-en-0.22.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/content')
zip_ref.close()

# recase and punctuation

cased = subprocess.check_output('python vosk-recasepunc-en-0.22/recasepunc.py predict vosk-recasepunc-en-0.22/checkpoint', shell=True, text=True, input=text)

cased

# Create speech recognition function

def voice_recognition(filename):
    # model = Model(model_name="vosk-model-en-us-0.22")
    model = Model(model_name="vosk-model-small-en-us-0.15")
    # For a smaller download size, use model = Model(model_name="vosk-model-small-en-us-0.15")
    rec = KaldiRecognizer(model, FRAME_RATE)
    rec.SetWords(True)
    
    mp3 = AudioSegment.from_mp3(filename)
    mp3 = mp3.set_channels(CHANNELS)
    mp3 = mp3.set_frame_rate(FRAME_RATE)
    
    step = 45000
    transcript = ""
    for i in range(0, len(mp3), step):
        print(f"Progress: {i/len(mp3)}")
        segment = mp3[i:i+step]
        rec.AcceptWaveform(segment.raw_data)
        result = rec.Result()
        text = json.loads(result)["text"]
        transcript += text
    
    cased = subprocess.check_output('python vosk-recasepunc-en-0.22/recasepunc.py predict vosk-recasepunc-en-0.22/checkpoint', shell=True, text=True, input=transcript)
    return cased

# Trancribe bigger mp3 file

transcript = voice_recognition("/content/marketplace_full.mp3")

# summarization of transcribed audio
summarizer = pipeline("summarization")
# summarizer = pipeline("summarization", model="t5-small")
# For a smaller model, use: summarizer = pipeline("summarization", model="t5-small")

split_tokens = transcript.split(" ")
docs = []
for i in range(0, len(split_tokens), 850):
    selection = " ".join(split_tokens[i:(i+850)])
    docs.append(selection)

summaries = summarizer(docs)

summary = "\n\n".join([d["summary_text"] for d in summaries])

print(summary)

"""## Microphone live speech recognition

- Note: This may only work on local system since it needs access to microphone
"""

# Find local microphone index

p = pyaudio.PyAudio()
p.get_device_count()

for i in range(p.get_device_count()):
    print(p.get_device_info_by_index(i).get('name'))

# Function to record from microphone

def record_microphone(seconds=10, chunk=1024, audio_format=pyaudio.paInt16):
    p = pyaudio.PyAudio()

    stream = p.open(format=audio_format,
                    channels=CHANNELS,
                    rate=FRAME_RATE,
                    input=True,
                    input_device_index=2, # match this index to your local microphone index
                    frames_per_buffer=chunk)

    frames = []

    for i in range(0, int(FRAME_RATE / chunk * seconds)):
        data = stream.read(chunk)
        frames.append(data)

    stream.stop_stream()
    stream.close()
    p.terminate()

    sound = AudioSegment(
        data=b''.join(frames),
        sample_width=p.get_sample_size(audio_format),
        frame_rate=FRAME_RATE,
        channels=CHANNELS
    )
    sound.export("temp.mp3", "mp3")

record_microphone()

# Create buttons for start and stop recording and transcribe/summarize live recording from microphone

record_button = widgets.Button(
    description='Record',
    disabled=False,
    button_style='success',
    tooltip='Record',
    icon='microphone'
)

summary = widgets.Output()

def start_recording(data):
    with summary:
        display("Starting the recording.")
        record_microphone()
        display("Finished recording.")
        transcript = voice_recognition("temp.mp3")
        display(f"Transcript: {transcript}")

record_button.on_click(start_recording)

display(record_button, summary)